{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# openNanoBanana -- Free Colab Edition\n\nThe full openNanoBanana pipeline running on a free Google Colab T4 GPU.\n\n**Pipeline:**\n1. Gemini 3 Flash extracts the real-world subject from your prompt\n2. Serper.dev searches Google Images for reference photos\n3. Gemini 3 Flash verifies the images match the subject\n4. FLUX.2-klein-4B-FP8 generates the final image locally on your T4 GPU\n\n**Before you start:**\n- Runtime > Change runtime type > **T4 GPU**\n- Paste your API keys in Cell 2\n\n**Get API keys (free):**\n- Gemini: [aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)\n- Serper: [serper.dev](https://serper.dev) (2,500 free queries/month, no credit card)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 1: Check GPU + Install Dependencies\n!nvidia-smi\n\nimport torch\nprint(f\"\\nPyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    print(f\"Compute capability: {torch.cuda.get_device_capability()}\")\n\nprint(\"\\nInstalling dependencies...\")\n!pip install -q git+https://github.com/huggingface/diffusers.git\n!pip install -q transformers accelerate safetensors\n!pip install -q bitsandbytes\n!pip install -q gradio Pillow requests\nprint(\"\\nDone!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 2: Configure API Keys\n# Paste your API keys below. Get them for free:\n#   Gemini: https://aistudio.google.com/app/apikey\n#   Serper: https://serper.dev (no credit card needed)\n\nGEMINI_API_KEY = \"\"  # <-- paste your Gemini API key here\nSERPER_API_KEY = \"\"  # <-- paste your Serper API key here\n\n# Optional: HuggingFace token for faster model downloads\nHF_TOKEN = \"\"  # <-- paste your HF token here (or leave empty)\n\nif HF_TOKEN:\n    from huggingface_hub import login\n    login(token=HF_TOKEN, add_to_git_credential=False)\n    print(\"Logged in to HuggingFace\")\n\nassert GEMINI_API_KEY, \"Please paste your Gemini API key above\"\nassert SERPER_API_KEY, \"Please paste your Serper API key above\"\nprint(\"API keys configured!\")\n\nGEMINI_MODEL = \"gemini-3-flash-preview\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 3: Load FLUX.2-klein-4B with 4-bit quantization (BOTH transformer AND text encoder)\n#\n# Why previous approaches crashed:\n#   - Full fp16: transformer (8GB) + text encoder (8GB) = 16GB CPU RAM > 12.7GB → OOM\n#   - FP8 conversion: FP8→fp16 cast = ~12GB CPU RAM peak → OOM\n#   - Transformer-only NF4: transformer on GPU (2GB) BUT text encoder fp16 on CPU (8GB)\n#     → System RAM 12.1/12.7 GB → crashes during inference\n#\n# Solution: Quantize BOTH to 4-bit NF4. Both stay on GPU permanently.\n#   - Transformer NF4: ~2 GB GPU\n#   - Text encoder NF4: ~2 GB GPU\n#   - VAE fp16: ~0.5 GB (offloaded to CPU, moved to GPU only during decode)\n#   - Total: ~4 GB GPU, <1 GB CPU → massive headroom on both\n\nimport torch, gc\n\ndtype = torch.float16\n\nfrom diffusers import Flux2KleinPipeline, Flux2Transformer2DModel, BitsAndBytesConfig\nfrom transformers import Qwen3ForCausalLM, BitsAndBytesConfig as TfBnBConfig\n\n# NF4 configs\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n)\ntf_nf4_config = TfBnBConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n)\n\n# ── Step 1: Transformer → NF4 on GPU (~2 GB) ─────────────────────\nprint(\"Step 1/3: Loading transformer with 4-bit NF4 quantization...\")\ntransformer = Flux2Transformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.2-klein-4B\",\n    subfolder=\"transformer\",\n    quantization_config=nf4_config,\n    torch_dtype=dtype,\n)\ngc.collect()\nprint(f\"  Transformer on GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n\n# ── Step 2: Text encoder → NF4 on GPU (~2 GB) ────────────────────\n# This was the crash cause: at fp16 it took ~8 GB of CPU RAM.\n# NF4 puts it on GPU at ~2 GB instead.\nprint(\"\\nStep 2/3: Loading text encoder with 4-bit NF4 quantization...\")\ntext_encoder = Qwen3ForCausalLM.from_pretrained(\n    \"black-forest-labs/FLUX.2-klein-4B\",\n    subfolder=\"text_encoder\",\n    quantization_config=tf_nf4_config,\n    torch_dtype=dtype,\n)\ngc.collect()\nprint(f\"  Transformer + text encoder on GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n\n# ── Step 3: Assemble pipeline (only downloads VAE, tokenizer, scheduler) ──\nprint(\"\\nStep 3/3: Loading VAE, tokenizer, scheduler...\")\npipe = Flux2KleinPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.2-klein-4B\",\n    transformer=transformer,\n    text_encoder=text_encoder,\n    torch_dtype=dtype,\n)\n\npipe.enable_model_cpu_offload()\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f\"\\nModel loaded!\")\nprint(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\nprint(f\"GPU memory reserved:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\nprint(f\"System RAM is now free — both big models live on GPU at 4-bit.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 4: Pipeline Functions\n# Python port of the openNanoBanana TypeScript pipeline\n\nimport requests\nimport json\nimport base64\nimport time\nimport re\nimport torch\nfrom io import BytesIO\nfrom PIL import Image\n\nGEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/models\"\nSERPER_URL = \"https://google.serper.dev/images\"\n\nRESOLUTION_MAP = {\n    \"1k\": (512, 512),\n    \"2k\": (768, 768),\n}\n\nEXTRACT_PROMPT = \"\"\"You are a search query extractor. Given an image generation prompt, extract TWO things:\n\n1. **searchQuery**: The real-world subject to search for as a reference image. Remove artistic style descriptors, effects, transformations, or hypothetical modifiers (e.g. \"as a baby\", \"in cyberpunk style\", \"watercolor painting\").\n2. **subjectType**: A SHORT, generic visual description of what the subject IS -- the kind of thing a person could identify by looking at a photo (e.g. \"a person\", \"a bridge\", \"a building\", \"a cat\", \"a street crossing\"). Do NOT use the specific name. This is used to verify search results visually.\n\nReturn ONLY valid JSON, no markdown fences, no explanation.\n\nExamples:\n- \"hkust entrance piazza in cyberpunk future\" -> {\"searchQuery\":\"hkust entrance piazza\",\"subjectType\":\"an outdoor plaza at a university\"}\n- \"golden gate bridge at sunset watercolor painting\" -> {\"searchQuery\":\"golden gate bridge\",\"subjectType\":\"a suspension bridge\"}\n- \"dr ct abraham as a baby\" -> {\"searchQuery\":\"dr ct abraham\",\"subjectType\":\"a person\"}\n- \"my cat wearing a top hat in van gogh style\" -> {\"searchQuery\":\"cat wearing top hat\",\"subjectType\":\"a cat\"}\n- \"tokyo shibuya crossing in anime style\" -> {\"searchQuery\":\"tokyo shibuya crossing\",\"subjectType\":\"a busy street crossing\"}\n- \"labrador puppy in a spacesuit\" -> {\"searchQuery\":\"labrador puppy\",\"subjectType\":\"a dog\"}\"\"\"\n\n\n# ===== Gemini Client =====\n\ndef call_gemini(api_key, model, contents):\n    \"\"\"Call Gemini API and return the text response.\"\"\"\n    url = f\"{GEMINI_BASE_URL}/{model}:generateContent\"\n    res = requests.post(\n        url,\n        headers={\"x-goog-api-key\": api_key, \"Content-Type\": \"application/json\"},\n        json={\"contents\": contents},\n        timeout=30,\n    )\n    if res.status_code == 429:\n        raise Exception(\"Gemini rate limit hit. Wait a moment and try again.\")\n    if res.status_code in (401, 403):\n        raise Exception(\"Invalid Gemini API key.\")\n    if not res.ok:\n        raise Exception(f\"Gemini API error ({res.status_code}): {res.text[:200]}\")\n\n    data = res.json()\n    candidate = (data.get(\"candidates\") or [None])[0]\n\n    if not candidate:\n        block = data.get(\"promptFeedback\", {}).get(\"blockReason\")\n        if block:\n            raise Exception(f\"Content blocked by Gemini safety filters: {block}\")\n        raise Exception(\"Gemini returned no candidates.\")\n\n    if candidate.get(\"finishReason\") == \"SAFETY\":\n        raise Exception(\"Content blocked by Gemini safety filters.\")\n\n    text = (candidate.get(\"content\", {}).get(\"parts\") or [{}])[0].get(\"text\")\n    if not isinstance(text, str):\n        raise Exception(\"Gemini returned no text in response.\")\n    return text\n\n\n# ===== Step 1: Extract Search Query =====\n\ndef extract_search_query(api_key, model, user_prompt):\n    \"\"\"Extract search query and subject type from user prompt. Returns (query, subject_type).\"\"\"\n    text = call_gemini(api_key, model, [\n        {\"parts\": [{\"text\": EXTRACT_PROMPT}, {\"text\": f'User prompt: \"{user_prompt}\"'}]}\n    ])\n    cleaned = re.sub(r\"^```json\\s*|```\\s*$\", \"\", text.strip()).strip()\n    try:\n        parsed = json.loads(cleaned)\n    except json.JSONDecodeError:\n        raise Exception(f\"Could not parse extraction result: {cleaned[:200]}\")\n\n    query = (parsed.get(\"searchQuery\") or \"\").strip().strip('\"\\'')\n    subject_type = (parsed.get(\"subjectType\") or \"a subject\").strip()\n\n    if not query or len(query) < 2:\n        raise Exception(\"Could not extract a search query from the prompt.\")\n    return query, subject_type\n\n\n# ===== Step 2: Search Images =====\n\ndef search_images(api_key, query, num=5):\n    \"\"\"Search Google Images via Serper. Returns list of {title, imageUrl, link}.\"\"\"\n    res = requests.post(\n        SERPER_URL,\n        headers={\"X-API-KEY\": api_key, \"Content-Type\": \"application/json\"},\n        json={\"q\": query, \"num\": num},\n        timeout=15,\n    )\n    if res.status_code == 429:\n        raise Exception(\"Serper rate limit hit.\")\n    if res.status_code in (401, 403):\n        raise Exception(\"Invalid Serper API key.\")\n    if not res.ok:\n        raise Exception(f\"Serper API error ({res.status_code}): {res.text[:200]}\")\n\n    data = res.json()\n    images = [\n        {\"title\": img.get(\"title\", \"\"), \"imageUrl\": img.get(\"imageUrl\", \"\"), \"link\": img.get(\"link\", \"\")}\n        for img in data.get(\"images\", [])\n    ]\n    if not images:\n        raise Exception(f'No images found for \"{query}\". Try a more specific prompt.')\n    return images\n\n\n# ===== Image Download =====\n\nMIME_MAP = {\".jpg\": \"image/jpeg\", \".jpeg\": \"image/jpeg\", \".png\": \"image/png\", \".webp\": \"image/webp\", \".gif\": \"image/gif\"}\n\ndef infer_mime(url, content_type=None):\n    if content_type and content_type.startswith(\"image/\"):\n        return content_type.split(\";\")[0].strip()\n    ext_match = re.search(r'\\.\\w+$', url.split(\"?\")[0])\n    if ext_match:\n        return MIME_MAP.get(ext_match.group(0).lower(), \"image/jpeg\")\n    return \"image/jpeg\"\n\ndef fetch_image_as_base64(url, timeout=8):\n    \"\"\"Download image and return (base64_str, mime_type) or None.\"\"\"\n    try:\n        res = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"openNanoBanana/1.0\"})\n        if not res.ok:\n            return None\n        ct = res.headers.get(\"Content-Type\", \"\")\n        if not ct.startswith(\"image/\"):\n            return None\n        if len(res.content) < 100:\n            return None\n        b64 = base64.b64encode(res.content).decode(\"utf-8\")\n        mime = infer_mime(url, ct)\n        return b64, mime\n    except Exception:\n        return None\n\ndef download_pil_image(url, timeout=10):\n    \"\"\"Download image and return PIL Image or None.\"\"\"\n    try:\n        res = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"openNanoBanana/1.0\"})\n        res.raise_for_status()\n        return Image.open(BytesIO(res.content)).convert(\"RGB\")\n    except Exception:\n        return None\n\n\n# ===== Step 3: Verify Images =====\n\ndef verify_images(api_key, model, subject_type, image_results):\n    \"\"\"Check each image with Gemini. Returns (imageUrl, base64, mime) of first match, or None.\"\"\"\n    for i, img in enumerate(image_results):\n        result = fetch_image_as_base64(img[\"imageUrl\"])\n        if result is None:\n            continue\n        b64, mime = result\n        try:\n            answer = call_gemini(api_key, model, [{\n                \"parts\": [\n                    {\"inline_data\": {\"mime_type\": mime, \"data\": b64}},\n                    {\"text\": f'Does this image clearly contain {subject_type}? Answer with ONLY \"yes\" or \"no\".'},\n                ]\n            }])\n            if answer.strip().lower().startswith(\"yes\"):\n                return {\"imageUrl\": img[\"imageUrl\"], \"base64\": b64, \"mimeType\": mime, \"index\": i}\n        except Exception:\n            continue\n    return None\n\n\n# ===== Step 4: Generate Image Locally =====\n\ndef generate_image(flux_pipe, prompt, reference_image_url, resolution=\"1k\"):\n    \"\"\"Generate image using local FLUX.2 pipeline. Returns PIL Image.\"\"\"\n    h, w = RESOLUTION_MAP.get(resolution, (512, 512))\n\n    ref_img = download_pil_image(reference_image_url)\n    if ref_img is None:\n        raise Exception(\"Failed to download reference image for generation.\")\n    ref_img = ref_img.resize((w, h))\n\n    with torch.inference_mode():\n        result = flux_pipe(\n            prompt=prompt,\n            image=[ref_img],\n            height=h,\n            width=w,\n            guidance_scale=1.0,\n            num_inference_steps=4,\n            generator=torch.Generator(device=\"cpu\").manual_seed(int(time.time()) % 2**32),\n        )\n    torch.cuda.empty_cache()\n    return result.images[0]\n\n\n# ===== Full Pipeline Orchestrator =====\n\ndef run_pipeline(prompt, gemini_key, serper_key, flux_pipe, model=GEMINI_MODEL, resolution=\"1k\"):\n    \"\"\"\n    Run the full openNanoBanana pipeline.\n    Returns (final_image, gallery_images, verified_index, log_text).\n    \"\"\"\n    log = []\n\n    # Step 1: Extract search query\n    log.append(\"[1/4] Extracting search query...\")\n    query, subject_type = extract_search_query(gemini_key, model, prompt)\n    log.append(f'  Search query: \"{query}\"')\n    log.append(f'  Subject type: \"{subject_type}\"')\n\n    # Step 2: Search images\n    log.append(f'\\n[2/4] Searching Google Images for \"{query}\"...')\n    results = search_images(serper_key, query, num=5)\n    log.append(f\"  Found {len(results)} candidate images\")\n\n    # Download thumbnails for gallery\n    gallery_images = []\n    for img in results:\n        pil = download_pil_image(img[\"imageUrl\"])\n        if pil:\n            gallery_images.append((pil, img[\"title\"][:60]))\n        else:\n            gallery_images.append(None)\n\n    # Step 3: Verify images\n    log.append(\"\\n[3/4] Verifying images match the subject...\")\n    verified = verify_images(gemini_key, model, subject_type, results)\n    if not verified:\n        raise Exception(\"None of the found images match the subject. Try rephrasing your prompt.\")\n    log.append(f'  Verified image #{verified[\"index\"] + 1}: {results[verified[\"index\"]][\"title\"][:60]}')\n\n    # Step 4: Generate\n    log.append(\"\\n[4/4] Generating image with FLUX.2-klein-4B (this may take 10-30s)...\")\n    final_image = generate_image(flux_pipe, prompt, verified[\"imageUrl\"], resolution)\n    log.append(\"  Done!\")\n\n    # Filter out None entries from gallery\n    gallery_clean = [g for g in gallery_images if g is not None]\n\n    return final_image, gallery_clean, verified[\"index\"], \"\\n\".join(log)\n\n\nprint(\"Pipeline functions loaded.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Gradio UI\n",
    "import gradio as gr\n",
    "\n",
    "def generate_handler(prompt, resolution):\n",
    "    \"\"\"Gradio handler for the generate button.\"\"\"\n",
    "    if not GEMINI_API_KEY:\n",
    "        raise gr.Error(\"GEMINI_API_KEY not set. Add it to Colab Secrets.\")\n",
    "    if not SERPER_API_KEY:\n",
    "        raise gr.Error(\"SERPER_API_KEY not set. Add it to Colab Secrets.\")\n",
    "    if not prompt or len(prompt.strip()) < 3:\n",
    "        raise gr.Error(\"Please enter a prompt (at least 3 characters).\")\n",
    "\n",
    "    try:\n",
    "        final_image, gallery, verified_idx, log = run_pipeline(\n",
    "            prompt=prompt.strip(),\n",
    "            gemini_key=GEMINI_API_KEY,\n",
    "            serper_key=SERPER_API_KEY,\n",
    "            flux_pipe=pipe,\n",
    "            resolution=resolution,\n",
    "        )\n",
    "        return final_image, gallery, log\n",
    "    except Exception as e:\n",
    "        raise gr.Error(str(e))\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"openNanoBanana\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# openNanoBanana\")\n",
    "    gr.Markdown(\"Real-time grounded image generation. Type a prompt with a real-world subject -- the pipeline searches for reference images, verifies them, and generates a new image grounded in reality.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            prompt_input = gr.Textbox(\n",
    "                label=\"Prompt\",\n",
    "                placeholder=\"andrej karpathy in a gta v poster\",\n",
    "                lines=2,\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            resolution_input = gr.Dropdown(\n",
    "                choices=[\"1k\", \"2k\"],\n",
    "                value=\"1k\",\n",
    "                label=\"Resolution\",\n",
    "            )\n",
    "            generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            output_image = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
    "        with gr.Column():\n",
    "            gallery_output = gr.Gallery(label=\"Reference Images Found\", columns=3, height=300)\n",
    "            log_output = gr.Textbox(label=\"Pipeline Log\", lines=12, interactive=False)\n",
    "\n",
    "    generate_btn.click(\n",
    "        fn=generate_handler,\n",
    "        inputs=[prompt_input, resolution_input],\n",
    "        outputs=[output_image, gallery_output, log_output],\n",
    "    )\n",
    "\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"hkust entrance piazza in cyberpunk future\", \"1k\"],\n",
    "            [\"andrej karpathy in a gta v poster\", \"1k\"],\n",
    "            [\"golden gate bridge at sunset watercolor painting\", \"1k\"],\n",
    "            [\"tokyo shibuya crossing in anime style\", \"1k\"],\n",
    "        ],\n",
    "        inputs=[prompt_input, resolution_input],\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}