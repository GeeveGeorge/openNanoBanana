{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# openNanoBanana -- Free Colab Backend\n",
        "\n",
        "Run **FLUX.2-klein-4B** on a free Google Colab T4 GPU as the image generation backend for [openNanoBanana](https://github.com/GeeveGeorge/openNanoBanana).\n",
        "\n",
        "**What this does:**\n",
        "1. Loads FLUX.2-klein-4B (4B params, Apache 2.0, 4-step inference)\n",
        "2. Exposes a RunPod-compatible API via ngrok tunnel\n",
        "3. Your openNanoBanana app calls this instead of RunPod -- completely free\n",
        "\n",
        "**Before you start:**\n",
        "- Runtime > Change runtime type > **T4 GPU**\n",
        "- Add `NGROK_TOKEN` to Colab Secrets (get one free at [ngrok.com](https://ngrok.com))\n",
        "- Optionally add `HF_TOKEN` for faster model downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Check GPU + Install Dependencies\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")\n",
        "    print(f\"Compute capability: {torch.cuda.get_device_capability()}\")\n",
        "\n",
        "print(\"\\nInstalling dependencies...\")\n",
        "!pip install -q git+https://github.com/huggingface/diffusers.git\n",
        "!pip install -q transformers accelerate safetensors\n",
        "!pip install -q flask pyngrok Pillow requests\n",
        "print(\"\\nDone! All dependencies installed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Configure Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# ngrok auth token (REQUIRED for public API tunnel)\n",
        "try:\n",
        "    NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n",
        "    print(\"NGROK_TOKEN found in Colab Secrets\")\n",
        "except Exception:\n",
        "    NGROK_TOKEN = None\n",
        "    print(\"WARNING: NGROK_TOKEN not found in Colab Secrets!\")\n",
        "    print(\"Go to: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    print(\"Then: Click the key icon in Colab's left sidebar > Add NGROK_TOKEN\")\n",
        "\n",
        "# HuggingFace token (optional, for faster downloads)\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    from huggingface_hub import login\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    print(\"Logged in to HuggingFace\")\n",
        "except Exception:\n",
        "    print(\"No HF_TOKEN -- downloading anonymously (may be slower)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Load FLUX.2-klein-4B\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# T4 has excellent FP16 tensor cores (65 TFLOPS)\n",
        "# T4 does NOT have native BF16 cores -- BF16 would run at ~8 TFLOPS via FP32 emulation\n",
        "dtype = torch.float16\n",
        "\n",
        "print(\"Loading FLUX.2-klein-4B (this takes 2-4 minutes on first run)...\")\n",
        "print(\"The model will be cached for subsequent runs.\\n\")\n",
        "\n",
        "from diffusers import Flux2KleinPipeline\n",
        "\n",
        "pipe = Flux2KleinPipeline.from_pretrained(\n",
        "    \"black-forest-labs/FLUX.2-klein-4B\",\n",
        "    torch_dtype=dtype,\n",
        ")\n",
        "\n",
        "# CPU offloading: only the active submodule (transformer/text_encoder/VAE) is on GPU at a time\n",
        "# Peak GPU usage: ~4 GB (transformer) instead of ~8 GB (everything)\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nModel loaded!\")\n",
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(f\"GPU memory reserved:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Test Generation\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "import time\n",
        "\n",
        "# --- Test 1: Text-to-image ---\n",
        "print(\"Test 1: Text-to-image (512x512, 4 steps)...\")\n",
        "t0 = time.time()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    image = pipe(\n",
        "        prompt=\"A cat holding a sign that says hello world\",\n",
        "        height=512,\n",
        "        width=512,\n",
        "        guidance_scale=4.0,\n",
        "        num_inference_steps=4,\n",
        "        generator=torch.Generator(device=\"cpu\").manual_seed(42),\n",
        "    ).images[0]\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f\"Generated in {elapsed:.1f}s\")\n",
        "print(f\"GPU memory: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB peak\")\n",
        "display(image)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# --- Test 2: Image editing with reference ---\n",
        "print(\"\\nTest 2: Image editing with reference image (512x512, 4 steps)...\")\n",
        "ref_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
        "resp = requests.get(ref_url, timeout=15)\n",
        "ref_image = Image.open(BytesIO(resp.content)).convert(\"RGB\").resize((512, 512))\n",
        "\n",
        "t0 = time.time()\n",
        "with torch.inference_mode():\n",
        "    edited = pipe(\n",
        "        prompt=\"A cat in a cyberpunk city at night with neon lights\",\n",
        "        image=[ref_image],\n",
        "        height=512,\n",
        "        width=512,\n",
        "        guidance_scale=4.0,\n",
        "        num_inference_steps=4,\n",
        "        generator=torch.Generator(device=\"cpu\").manual_seed(42),\n",
        "    ).images[0]\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f\"Generated in {elapsed:.1f}s\")\n",
        "print(f\"GPU memory: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB peak\")\n",
        "display(edited)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\nBoth tests passed! Model is working correctly.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Start API Server with ngrok tunnel\n",
        "import threading\n",
        "import base64\n",
        "import uuid\n",
        "import time\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "import requests as http_requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from flask import Flask, request as flask_request, jsonify\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Resolution mapping -- conservative for T4 VRAM\n",
        "RESOLUTION_MAP = {\n",
        "    \"1k\": (512, 512),\n",
        "    \"2k\": (768, 768),\n",
        "}\n",
        "\n",
        "# In-memory job store for async /run + /status endpoints\n",
        "jobs = {}\n",
        "# Lock to prevent concurrent GPU usage (T4 can only run one inference at a time)\n",
        "gpu_lock = threading.Lock()\n",
        "\n",
        "\n",
        "def _download_images(urls, width, height):\n",
        "    \"\"\"Download and resize reference images.\"\"\"\n",
        "    images = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            resp = http_requests.get(url, timeout=15)\n",
        "            resp.raise_for_status()\n",
        "            img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
        "            img = img.resize((width, height))\n",
        "            images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"  Failed to download {url[:80]}: {e}\")\n",
        "    return images\n",
        "\n",
        "\n",
        "def _generate(input_data):\n",
        "    \"\"\"Run the FLUX.2 pipeline. Returns dict with 'result' key.\"\"\"\n",
        "    prompt = input_data.get(\"prompt\", \"\")\n",
        "    image_urls = input_data.get(\"images\", [])\n",
        "    resolution = input_data.get(\"resolution\", \"1k\")\n",
        "    output_format = input_data.get(\"output_format\", \"jpeg\")\n",
        "\n",
        "    if not prompt:\n",
        "        raise ValueError(\"prompt is required\")\n",
        "\n",
        "    h, w = RESOLUTION_MAP.get(resolution, (512, 512))\n",
        "\n",
        "    # Download reference images\n",
        "    ref_images = _download_images(image_urls, w, h)\n",
        "\n",
        "    # Build pipeline kwargs\n",
        "    pipe_kwargs = dict(\n",
        "        prompt=prompt,\n",
        "        height=h,\n",
        "        width=w,\n",
        "        guidance_scale=4.0,\n",
        "        num_inference_steps=4,\n",
        "        generator=torch.Generator(device=\"cpu\").manual_seed(int(time.time()) % 2**32),\n",
        "    )\n",
        "    if ref_images:\n",
        "        pipe_kwargs[\"image\"] = ref_images\n",
        "\n",
        "    # Run inference with GPU lock\n",
        "    with gpu_lock:\n",
        "        with torch.inference_mode():\n",
        "            result = pipe(**pipe_kwargs)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Encode output to base64\n",
        "    output_image = result.images[0]\n",
        "    buffer = BytesIO()\n",
        "    fmt = \"JPEG\" if output_format.lower() in (\"jpeg\", \"jpg\") else \"PNG\"\n",
        "    output_image.save(buffer, format=fmt, quality=90)\n",
        "    b64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    data_uri = f\"data:image/{fmt.lower()};base64,{b64}\"\n",
        "\n",
        "    return {\"result\": data_uri}\n",
        "\n",
        "\n",
        "# ===== Health endpoint =====\n",
        "@app.route(\"/health\", methods=[\"GET\"])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        \"status\": \"ok\",\n",
        "        \"model\": \"FLUX.2-klein-4B\",\n",
        "        \"gpu\": torch.cuda.get_device_name() if torch.cuda.is_available() else \"none\",\n",
        "        \"vram_allocated_gb\": round(torch.cuda.memory_allocated() / 1024**3, 2),\n",
        "    })\n",
        "\n",
        "\n",
        "# ===== Synchronous generate endpoint =====\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    try:\n",
        "        data = flask_request.get_json()\n",
        "        result = _generate(data)\n",
        "        return jsonify(result)\n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        torch.cuda.empty_cache()\n",
        "        return jsonify({\"error\": \"GPU out of memory. Try resolution '1k'.\"}), 503\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "# ===== RunPod-compatible async endpoints =====\n",
        "# These mimic RunPod's /run + /status/{id} API so the openNanoBanana\n",
        "# Next.js app can use them with minimal changes.\n",
        "\n",
        "@app.route(\"/run\", methods=[\"POST\"])\n",
        "def run_async():\n",
        "    data = flask_request.get_json()\n",
        "    input_data = data.get(\"input\", data)\n",
        "    job_id = str(uuid.uuid4())\n",
        "    jobs[job_id] = {\"status\": \"IN_QUEUE\", \"output\": None, \"error\": None}\n",
        "\n",
        "    def process():\n",
        "        jobs[job_id][\"status\"] = \"IN_PROGRESS\"\n",
        "        try:\n",
        "            output = _generate(input_data)\n",
        "            jobs[job_id][\"output\"] = output\n",
        "            jobs[job_id][\"status\"] = \"COMPLETED\"\n",
        "        except Exception as e:\n",
        "            jobs[job_id][\"error\"] = str(e)\n",
        "            jobs[job_id][\"status\"] = \"FAILED\"\n",
        "\n",
        "    threading.Thread(target=process, daemon=True).start()\n",
        "    return jsonify({\"id\": job_id, \"status\": \"IN_QUEUE\"})\n",
        "\n",
        "\n",
        "@app.route(\"/status/<job_id>\", methods=[\"GET\"])\n",
        "def status(job_id):\n",
        "    job = jobs.get(job_id)\n",
        "    if not job:\n",
        "        return jsonify({\"error\": \"Job not found\"}), 404\n",
        "    response = {\"id\": job_id, \"status\": job[\"status\"]}\n",
        "    if job[\"output\"]:\n",
        "        response[\"output\"] = job[\"output\"]\n",
        "    if job[\"error\"]:\n",
        "        response[\"error\"] = job[\"error\"]\n",
        "    return jsonify(response)\n",
        "\n",
        "\n",
        "# ===== Start server + tunnel =====\n",
        "if not NGROK_TOKEN:\n",
        "    print(\"ERROR: NGROK_TOKEN is not set. Cannot create public tunnel.\")\n",
        "    print(\"Add it to Colab Secrets (key icon in left sidebar).\")\n",
        "else:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(f\"  API is live!\")\n",
        "    print(f\"\")\n",
        "    print(f\"  Base URL: {public_url}\")\n",
        "    print(f\"\")\n",
        "    print(f\"  Endpoints:\")\n",
        "    print(f\"    GET  {public_url}/health\")\n",
        "    print(f\"    POST {public_url}/generate  (synchronous)\")\n",
        "    print(f\"    POST {public_url}/run       (async, RunPod-compatible)\")\n",
        "    print(f\"    GET  {public_url}/status/<id>\")\n",
        "    print(f\"\")\n",
        "    print(f\"  To use with openNanoBanana:\")\n",
        "    print(f\"    Set COLAB_URL={public_url} in your .env.local\")\n",
        "    print(f\"    Or paste this URL in the BYOK panel\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Run Flask in background thread so cell doesn't block\n",
        "    threading.Thread(\n",
        "        target=lambda: app.run(port=5000, use_reloader=False),\n",
        "        daemon=True,\n",
        "    ).start()\n",
        "\n",
        "    print(\"\\nServer running. Keep this cell alive -- it serves requests in the background.\")\n",
        "    print(\"The URL will change if you restart the notebook.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to use with openNanoBanana\n",
        "\n",
        "1. Copy the **Base URL** printed above (e.g. `https://xxxx-xx-xx.ngrok-free.app`)\n",
        "\n",
        "2. **Option A -- Environment variable:**\n",
        "   ```bash\n",
        "   # Add to your .env.local\n",
        "   COLAB_URL=https://xxxx-xx-xx.ngrok-free.app\n",
        "   ```\n",
        "\n",
        "3. **Option B -- BYOK panel:**\n",
        "   Paste the URL in the \"Colab URL\" field in the openNanoBanana web UI\n",
        "\n",
        "### Limitations\n",
        "\n",
        "| Constraint | Detail |\n",
        "|---|---|\n",
        "| Session duration | Free Colab disconnects after ~90 min idle or ~12 hours total |\n",
        "| URL changes | ngrok URL changes every time you restart. Copy the new one. |\n",
        "| Resolution | 512x512 recommended. 768x768 may work. 1024x1024 will likely OOM. |\n",
        "| Concurrency | One request at a time (T4 can only run one inference) |\n",
        "| Speed | ~8-20 seconds per image (T4 is slower than A100/H100) |\n",
        "\n",
        "### API Reference\n",
        "\n",
        "**POST /generate** (synchronous)\n",
        "```json\n",
        "{\n",
        "  \"prompt\": \"A cat in cyberpunk city\",\n",
        "  \"images\": [\"https://example.com/reference.jpg\"],\n",
        "  \"resolution\": \"1k\",\n",
        "  \"output_format\": \"jpeg\"\n",
        "}\n",
        "// Response: { \"result\": \"data:image/jpeg;base64,...\" }\n",
        "```\n",
        "\n",
        "**POST /run** (async, RunPod-compatible)\n",
        "```json\n",
        "{ \"input\": { \"prompt\": \"...\", \"images\": [\"...\"], \"resolution\": \"1k\" } }\n",
        "// Response: { \"id\": \"job-uuid\", \"status\": \"IN_QUEUE\" }\n",
        "```\n",
        "\n",
        "**GET /status/{job_id}**\n",
        "```json\n",
        "// Response: { \"id\": \"job-uuid\", \"status\": \"COMPLETED\", \"output\": { \"result\": \"data:...\" } }\n",
        "```"
      ]
    }
  ]
}